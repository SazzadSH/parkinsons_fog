{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":41880,"databundleVersionId":5677426,"sourceType":"competition"},{"sourceId":121731966,"sourceType":"kernelVersion"}],"dockerImageVersionId":30408,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport time\n\nimport json\nfrom tqdm import tqdm\nimport glob\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom sklearn.model_selection import train_test_split, StratifiedGroupKFold\nfrom sklearn.metrics import accuracy_score, average_precision_score\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-18T03:55:05.567357Z","iopub.execute_input":"2024-12-18T03:55:05.567747Z","iopub.status.idle":"2024-12-18T03:55:08.344405Z","shell.execute_reply.started":"2024-12-18T03:55:05.567710Z","shell.execute_reply":"2024-12-18T03:55:08.343601Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class Config:\n    train_dir1 = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog\"\n    train_dir2 = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog\"\n\n    batch_size = 512\n    window_size = 128\n    window_future = 8\n    window_past = window_size - window_future\n    \n    wx = 8\n    \n    lr = 0.00015\n    num_epochs = 8\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    feature_list = ['AccV', 'AccML', 'AccAP']\n    label_list = ['StartHesitation', 'Turn', 'Walking']\n    \n    \ncfg = Config()","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:08.346409Z","iopub.execute_input":"2024-12-18T03:55:08.347235Z","iopub.status.idle":"2024-12-18T03:55:08.401630Z","shell.execute_reply.started":"2024-12-18T03:55:08.347197Z","shell.execute_reply":"2024-12-18T03:55:08.400622Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"cfg.device","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:08.402814Z","iopub.execute_input":"2024-12-18T03:55:08.403357Z","iopub.status.idle":"2024-12-18T03:55:08.415369Z","shell.execute_reply.started":"2024-12-18T03:55:08.403329Z","shell.execute_reply":"2024-12-18T03:55:08.414540Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Analysis of positive instances in each fold of our CV folds\n\nn1_sum = []\nn2_sum = []\nn3_sum = []\ncount = []\n\n# Here I am using the metadata file available during training. Since the code will run again during submission, if \n# I used the usual file from the competition folder, it would have been updated with the test files too.\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/tdcsfog_metadata.csv\")\n\nfor f in tqdm(metadata['Id']):\n    fpath = f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{f}.csv\"\n    df = pd.read_csv(fpath)\n    \n    n1_sum.append(np.sum(df['StartHesitation']))\n    n2_sum.append(np.sum(df['Turn']))\n    n3_sum.append(np.sum(df['Walking']))\n    count.append(len(df))\n    \nprint(f\"32 files have positive values in all 3 classes\")\n\nmetadata['n1_sum'] = n1_sum\nmetadata['n2_sum'] = n2_sum\nmetadata['n3_sum'] = n3_sum\nmetadata['count'] = count\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    \n    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n# FOLD 2 is the most well balanced\n# The actual train-test split (based on Fold 2)\n\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/tdcsfog_metadata.csv\")\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    if i != 2:\n        continue\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n    \n    if i == 2:\n        break\n        \ntrain_fpaths_tdcs = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{_id}.csv\" for _id in train_ids]\nvalid_fpaths_tdcs = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{_id}.csv\" for _id in valid_ids]","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:08.417369Z","iopub.execute_input":"2024-12-18T03:55:08.417660Z","iopub.status.idle":"2024-12-18T03:55:33.643617Z","shell.execute_reply.started":"2024-12-18T03:55:08.417635Z","shell.execute_reply":"2024-12-18T03:55:33.642837Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 833/833 [00:25<00:00, 33.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"32 files have positive values in all 3 classes\nFold = 0\nLength of Train = 672, Length of Valid = 161\nTrain classes: 287,832, 1,462,652, 175,633\nValid classes: 16,958, 216,130, 32,205\nFold = 1\nLength of Train = 613, Length of Valid = 220\nTrain classes: 51,748, 909,505, 65,242\nValid classes: 253,042, 769,277, 142,596\nFold = 2\nLength of Train = 703, Length of Valid = 130\nTrain classes: 271,881, 1,332,746, 183,673\nValid classes: 32,909, 346,036, 24,165\nFold = 3\nLength of Train = 649, Length of Valid = 184\nTrain classes: 303,710, 1,517,147, 205,196\nValid classes: 1,080, 161,635, 2,642\nFold = 4\nLength of Train = 695, Length of Valid = 138\nTrain classes: 303,989, 1,493,078, 201,608\nValid classes: 801, 185,704, 6,230\nFold = 2\nLength of Train = 703, Length of Valid = 130\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### defog","metadata":{}},{"cell_type":"code","source":"# Analysis of positive instances in each fold of our CV folds\n\nn1_sum = []\nn2_sum = []\nn3_sum = []\ncount = []\n\n# Here I am using the metadata file available during training. Since the code will run again during submission, if \n# I used the usual file from the competition folder, it would have been updated with the test files too.\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/defog_metadata.csv\")\nmetadata['n1_sum'] = 0\nmetadata['n2_sum'] = 0\nmetadata['n3_sum'] = 0\nmetadata['count'] = 0\n\nfor f in tqdm(metadata['Id']):\n    fpath = f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{f}.csv\"\n    if os.path.exists(fpath) == False:\n        continue\n        \n    df = pd.read_csv(fpath)\n    metadata.loc[metadata['Id'] == f, 'n1_sum'] = np.sum(df['StartHesitation'])\n    metadata.loc[metadata['Id'] == f, 'n2_sum'] = np.sum(df['Turn'])\n    metadata.loc[metadata['Id'] == f, 'n3_sum'] = np.sum(df['Walking'])\n    metadata.loc[metadata['Id'] == f, 'count'] = len(df)\n    \nmetadata = metadata[metadata['count'] > 0].reset_index()\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    \n    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n# FOLD 2 is the most well balanced\n# The actual train-test split (based on Fold 2)\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    if i != 1:\n        continue\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n    \n    if i == 2:\n        break\n        \ntrain_fpaths_de = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{_id}.csv\" for _id in train_ids]\nvalid_fpaths_de = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{_id}.csv\" for _id in valid_ids]","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:33.645079Z","iopub.execute_input":"2024-12-18T03:55:33.645854Z","iopub.status.idle":"2024-12-18T03:55:59.829524Z","shell.execute_reply.started":"2024-12-18T03:55:33.645815Z","shell.execute_reply":"2024-12-18T03:55:59.828560Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 137/137 [00:26<00:00,  5.25it/s]","output_type":"stream"},{"name":"stdout","text":"Fold = 0\nLength of Train = 75, Length of Valid = 16\nTrain classes: 500, 428,683, 37,609\nValid classes: 0, 158,803, 60,910\nFold = 1\nLength of Train = 65, Length of Valid = 26\nTrain classes: 216, 490,429, 84,955\nValid classes: 284, 97,057, 13,564\nFold = 2\nLength of Train = 76, Length of Valid = 15\nTrain classes: 410, 488,634, 87,986\nValid classes: 90, 98,852, 10,533\nFold = 3\nLength of Train = 70, Length of Valid = 21\nTrain classes: 435, 424,494, 88,800\nValid classes: 65, 162,992, 9,719\nFold = 4\nLength of Train = 78, Length of Valid = 13\nTrain classes: 439, 517,704, 94,726\nValid classes: 61, 69,782, 3,793\nFold = 1\nLength of Train = 65, Length of Valid = 26\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_fpaths = [(f, 'de') for f in train_fpaths_de] + [(f, 'tdcs') for f in train_fpaths_tdcs]\nvalid_fpaths = [(f, 'de') for f in valid_fpaths_de] + [(f, 'tdcs') for f in valid_fpaths_tdcs]","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:59.830991Z","iopub.execute_input":"2024-12-18T03:55:59.831794Z","iopub.status.idle":"2024-12-18T03:55:59.836518Z","shell.execute_reply.started":"2024-12-18T03:55:59.831754Z","shell.execute_reply":"2024-12-18T03:55:59.835568Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class FOGDataset(Dataset):\n    def __init__(self, fpaths, scale=9.806, split=\"train\"):\n        super(FOGDataset, self).__init__()\n        tm = time.time()\n        self.split = split\n        self.scale = scale\n        \n        self.fpaths = fpaths\n        self.dfs = [self.read(f[0], f[1]) for f in fpaths]\n        self.f_ids = [os.path.basename(f[0])[:-4] for f in self.fpaths]\n        \n        self.end_indices = []\n        self.shapes = []\n        _length = 0\n        for df in self.dfs:\n            self.shapes.append(df.shape[0])\n            _length += df.shape[0]\n            self.end_indices.append(_length)\n        \n        self.dfs = np.concatenate(self.dfs, axis=0).astype(np.float16)\n        self.length = self.dfs.shape[0]\n        \n        shape1 = self.dfs.shape[1]\n        \n        self.dfs = np.concatenate([np.zeros((cfg.wx*cfg.window_past, shape1)), self.dfs, np.zeros((cfg.wx*cfg.window_future, shape1))], axis=0)\n        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n        gc.collect()\n        \n    def read(self, f, _type):\n        df = pd.read_csv(f)\n        if self.split == \"test\":\n            return np.array(df)\n        \n        if _type ==\"tdcs\":\n            df['Valid'] = 1\n            df['Task'] = 1\n            df['tdcs'] = 1\n        else:\n            df['tdcs'] = 0\n        \n        return np.array(df)\n            \n    def __getitem__(self, index):\n        if self.split == \"train\":\n            row_idx = random.randint(0, self.length-1) + cfg.wx*cfg.window_past\n        elif self.split == \"test\":\n            for i,e in enumerate(self.end_indices):\n                if index >= e:\n                    continue\n                df_idx = i\n                break\n\n            row_idx_true = self.shapes[df_idx] - (self.end_indices[df_idx] - index)\n            _id = self.f_ids[df_idx] + \"_\" + str(row_idx_true)\n            row_idx = index + cfg.wx*cfg.window_past\n        else:\n            row_idx = index + cfg.wx*cfg.window_past\n            \n        x = self.dfs[row_idx - cfg.wx*cfg.window_past : row_idx + cfg.wx*cfg.window_future, 1:4]\n        x = x[::cfg.wx, :][::-1, :]\n        x = torch.tensor(x.astype('float'))#/scale\n        \n        t = self.dfs[row_idx, -3]*self.dfs[row_idx, -2]\n        \n        if self.split == \"test\":\n            return _id, x, t\n        \n        y = self.dfs[row_idx, 4:7].astype('float')\n        y = torch.tensor(y)\n        \n        return x, y, t\n    \n    def __len__(self):\n        # return self.length\n        if self.split == \"train\":\n            return 5_000_000\n        return self.length","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:59.837788Z","iopub.execute_input":"2024-12-18T03:55:59.838151Z","iopub.status.idle":"2024-12-18T03:55:59.853267Z","shell.execute_reply.started":"2024-12-18T03:55:59.838115Z","shell.execute_reply":"2024-12-18T03:55:59.852271Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:59.854322Z","iopub.execute_input":"2024-12-18T03:55:59.854601Z","iopub.status.idle":"2024-12-18T03:55:59.987837Z","shell.execute_reply.started":"2024-12-18T03:55:59.854576Z","shell.execute_reply":"2024-12-18T03:55:59.986959Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.pad = nn.ConstantPad1d((7, 8), 0)\n        self.conv1_1 = nn.Conv1d(in_channels=3, out_channels=64, kernel_size=16, stride=1)\n        self.conv1_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=16, stride=1)\n        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.conv2_1 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=16, stride=1)\n        self.conv2_2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=16, stride=1)\n        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.conv3_1 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=16, stride=1)\n        self.conv3_2 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=16, stride=1)\n        self.conv3_3 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=16, stride=1)\n        \n        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.conv3a_1 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=16, stride=1)\n        self.conv3a_2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=16, stride=1)\n        self.conv3a_3 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=16, stride=1)\n        \n        self.upconv1 = nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n        self.conv4_1 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=16, stride=1)\n        self.conv4_2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=16, stride=1)\n        self.upconv2 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n        self.conv5_1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=16, stride=1)\n        self.conv5_2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=16, stride=1)\n        self.conv5_3 = nn.Conv1d(in_channels=64, out_channels=3, kernel_size=1, stride=1)\n        \n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 3)\n        \n    def forward(self, x):\n        x = np.swapaxes(x, 1, 2)\n        x = self.pad(x)\n        conv1_1 = self.conv1_1(x)\n        conv1_1 = self.pad(self.relu(conv1_1))\n        conv1_2 = self.conv1_2(conv1_1)\n        conv1_2 = self.relu(conv1_2)\n        pool1 = self.pool1(conv1_2)\n        pool1 = self.pad(pool1)\n        conv2_1 = self.conv2_1(pool1)\n        conv2_1 = self.pad(self.relu(conv2_1))\n        conv2_2 = self.conv2_2(conv2_1)\n        conv2_2 = self.relu(conv2_2)\n        pool2 = self.pool2(conv2_2)\n        pool2 = self.pad(pool2)\n        conv3_1 = self.conv3_1(pool2)\n        conv3_1 = self.pad(self.relu(conv3_1))\n        conv3_2 = self.conv3_2(conv3_1)\n        conv3_2 = self.pad(self.relu(conv3_2))\n        conv3_3 = self.conv3_3(conv3_2)\n        conv3_3 = self.relu(conv3_3)\n        \n        pool3 = self.pool3(conv3_3)\n        pool3 = self.pad(pool3)\n        conv3a_1 = self.conv3a_1(pool3)\n        conv3a_1 = self.pad(self.relu(conv3a_1))\n        conv3a_2 = self.conv3a_2(conv3a_1)\n        conv3a_2 = self.pad(self.relu(conv3a_2))\n        conv3a_3 = self.conv3a_3(conv3a_2)\n        conv3a_3 = self.relu(conv3a_3)\n        \n        conv5_3 = self.global_avg_pool(conv3a_3).squeeze(2)\n        fc1 = self.fc1(conv5_3)\n        fc1 = self.relu(fc1)\n        fc2 = self.fc2(fc1)\n        return fc2","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:55:59.989257Z","iopub.execute_input":"2024-12-18T03:55:59.989586Z","iopub.status.idle":"2024-12-18T03:56:00.008332Z","shell.execute_reply.started":"2024-12-18T03:55:59.989551Z","shell.execute_reply":"2024-12-18T03:56:00.007450Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:56:00.010339Z","iopub.execute_input":"2024-12-18T03:56:00.010604Z","iopub.status.idle":"2024-12-18T03:56:00.023806Z","shell.execute_reply.started":"2024-12-18T03:56:00.010580Z","shell.execute_reply":"2024-12-18T03:56:00.022961Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler\n\ndef train_one_epoch(model, loader, optimizer, criterion):\n    loss_sum = 0.\n    scaler = GradScaler()\n    \n    model.train()\n    for x,y,t in tqdm(loader):\n        x = x.to(cfg.device).float()\n        y = y.to(cfg.device).float()\n        t = t.to(cfg.device).float()\n        \n        y_pred = model(x)\n        loss = criterion(y_pred, y)\n        loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n        \n        t_sum = torch.sum(t)\n        if t_sum > 0:\n            loss = torch.sum(loss)/t_sum\n        else:\n            loss = torch.sum(loss)*0.\n        \n        # loss.backward()\n        scaler.scale(loss).backward()\n        # optimizer.step()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        optimizer.zero_grad()\n        \n        loss_sum += loss.item()\n    \n    print(f\"Train Loss: {(loss_sum/len(loader)):.04f}\")\n    \n\ndef validation_one_epoch(model, loader, criterion):\n    loss_sum = 0.\n    y_true_epoch = []\n    y_pred_epoch = []\n    t_valid_epoch = []\n    \n    model.eval()\n    for x,y,t in tqdm(loader):\n        x = x.to(cfg.device).float()\n        y = y.to(cfg.device).float()\n        t = t.to(cfg.device).float()\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n            \n            t_sum = torch.sum(t)\n            if t_sum > 0:\n                loss = torch.sum(loss)/t_sum\n            else:\n                loss = torch.sum(loss)*0.\n        \n        loss_sum += loss.item()\n        y_true_epoch.append(y.cpu().numpy())\n        y_pred_epoch.append(y_pred.cpu().numpy())\n        t_valid_epoch.append(t.cpu().numpy())\n        \n    y_true_epoch = np.concatenate(y_true_epoch, axis=0)\n    y_pred_epoch = np.concatenate(y_pred_epoch, axis=0)\n    \n    t_valid_epoch = np.concatenate(t_valid_epoch, axis=0)\n    y_true_epoch = y_true_epoch[t_valid_epoch > 0, :]\n    y_pred_epoch = y_pred_epoch[t_valid_epoch > 0, :]\n    \n    scores = [average_precision_score(y_true_epoch[:,i], y_pred_epoch[:,i]) for i in range(3)]\n    mean_score = np.mean(scores)\n    print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f},{scores[2]:.03f}\")\n    \n    return mean_score","metadata":{"execution":{"iopub.status.busy":"2024-12-18T03:56:00.024858Z","iopub.execute_input":"2024-12-18T03:56:00.025147Z","iopub.status.idle":"2024-12-18T03:56:00.039166Z","shell.execute_reply.started":"2024-12-18T03:56:00.025122Z","shell.execute_reply":"2024-12-18T03:56:00.038365Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = UNet().to(cfg.device)\nprint(f\"Number of parameters in model - {count_parameters(model):,}\")\n\ntrain_dataset = FOGDataset(train_fpaths, split=\"train\")\nvalid_dataset = FOGDataset(valid_fpaths, split=\"valid\")\nprint(f\"lengths of datasets: train - {len(train_dataset)}, valid - {len(valid_dataset)}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=5, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, num_workers=5)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='none').to(cfg.device)\n# sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n\nmax_score = 0.0\n\nprint(\"=\"*50)\nfor epoch in range(cfg.num_epochs):\n    print(f\"Epoch: {epoch}\")\n    train_one_epoch(model, train_loader, optimizer, criterion)\n    score = validation_one_epoch(model, valid_loader, criterion)\n    # sched.step()\n\n    if score > max_score:\n        max_score = score\n        torch.save(model.state_dict(), \"best_model_state.h5\")\n        print(\"Saving Model ...\")\n\n    print(\"=\"*50)\n    \ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}